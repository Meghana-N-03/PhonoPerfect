{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8966627,"sourceType":"datasetVersion","datasetId":5397614},{"sourceId":9163153,"sourceType":"datasetVersion","datasetId":5536169},{"sourceId":9409080,"sourceType":"datasetVersion","datasetId":5713232}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T06:12:19.095486Z","iopub.execute_input":"2024-08-20T06:12:19.096625Z","iopub.status.idle":"2024-08-20T06:12:19.104253Z","shell.execute_reply.started":"2024-08-20T06:12:19.096582Z","shell.execute_reply":"2024-08-20T06:12:19.103356Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/input/correct/capstone_dataset_hugging_face.csv\n/kaggle/input/accuracy94/phoneme_correction_model_editDistance_accuracy94.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\nfile_path = '/kaggle/input/correct/capstone_dataset_hugging_face.csv'  # Replace with the path to your CSV file\ndf = pd.read_csv(file_path)\n\n# Display the first few rows of the DataFrame\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T15:03:30.752433Z","iopub.execute_input":"2024-09-11T15:03:30.753006Z","iopub.status.idle":"2024-09-11T15:03:31.573317Z","shell.execute_reply.started":"2024-09-11T15:03:30.752978Z","shell.execute_reply":"2024-09-11T15:03:31.572457Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"     word ipa_phoneme incorrect_ipa_phoneme\n0      aa         ˈɑː                    ˈː\n1     aah         ˈɑː                    ˈɑ\n2   aahed        ˈɑːd                   ɑːd\n3  aahing       ˈɑːɪŋ                  ˈːɪŋ\n4    aahs        ˈɑːz                  ˈθːz\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the entire model\nmodel.save('phoneme_correction_model_editDistance.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T12:40:52.860285Z","iopub.execute_input":"2024-07-17T12:40:52.860695Z","iopub.status.idle":"2024-07-17T12:40:52.910080Z","shell.execute_reply.started":"2024-07-17T12:40:52.860653Z","shell.execute_reply":"2024-07-17T12:40:52.909319Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the model\nloaded_model = load_model('/kaggle/input/accuracy94/phoneme_correction_model_editDistance_accuracy94.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T15:03:50.291728Z","iopub.execute_input":"2024-09-11T15:03:50.292346Z","iopub.status.idle":"2024-09-11T15:03:51.561993Z","shell.execute_reply.started":"2024-09-11T15:03:50.292317Z","shell.execute_reply":"2024-09-11T15:03:51.561181Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"new_mispronounced_sequence = \"ɐbdˈʌks\"  # Replace with actual sequence\nnew_mispronounced_sequence_tokenized = tokenizer.texts_to_sequences([new_mispronounced_sequence])\nnew_mispronounced_sequence_padded = pad_sequences(new_mispronounced_sequence_tokenized, maxlen=max_sequence_length, padding='post')\n\n# Prepare the decoder input for inference\ndecoder_input = np.zeros_like(new_mispronounced_sequence_padded)\ndecoder_input[:, 1:] = new_mispronounced_sequence_padded[:, :-1]\n\n# Predict\npredicted_phonemes = loaded_model.predict([new_mispronounced_sequence_padded, decoder_input])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:12:26.483652Z","iopub.execute_input":"2024-08-20T06:12:26.484017Z","iopub.status.idle":"2024-08-20T06:12:28.051994Z","shell.execute_reply.started":"2024-08-20T06:12:26.483987Z","shell.execute_reply":"2024-08-20T06:12:28.051181Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","output_type":"stream"}]},{"cell_type":"code","source":"#94 accuracy - SEQ2SEQ WITH LSTM LAYERS ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding\nimport numpy as np\n\n# Load the CSV file\nfile_path = '/kaggle/input/correct/capstone_dataset_hugging_face.csv'  # Replace with the path to your CSV file\ndf = pd.read_csv(file_path)\n\n# Combine all phoneme sequences to fit the tokenizer\nall_phonemes = list(df['ipa_phoneme']) + list(df['incorrect_ipa_phoneme'])\n\ntokenizer = Tokenizer(char_level=True)  # Use character-level tokenization\ntokenizer.fit_on_texts(all_phonemes)\n\n# Create tokenized sequences\ncorrect_sequences = tokenizer.texts_to_sequences(df['ipa_phoneme'])\nincorrect_sequences = tokenizer.texts_to_sequences(df['incorrect_ipa_phoneme'])\n\n# Pad sequences to the same length\nmax_sequence_length = max(max(len(seq) for seq in correct_sequences), max(len(seq) for seq in incorrect_sequences))\n\ncorrect_sequences = pad_sequences(correct_sequences, maxlen=max_sequence_length, padding='post')\nincorrect_sequences = pad_sequences(incorrect_sequences, maxlen=max_sequence_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-09-25T15:03:41.551653Z","iopub.execute_input":"2024-09-25T15:03:41.552572Z","iopub.status.idle":"2024-09-25T15:04:04.328677Z","shell.execute_reply.started":"2024-09-25T15:03:41.552531Z","shell.execute_reply":"2024-09-25T15:04:04.327670Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-09-25 15:03:44.354857: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-25 15:03:44.355000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-25 15:03:44.477927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Split the data into training and validation sets\ntrain_incorrect, val_incorrect, train_correct, val_correct = train_test_split(\n    incorrect_sequences, correct_sequences, test_size=0.2, random_state=42\n)\n\n# Prepare decoder input sequences for training (shifted incorrect sequences)\ntrain_decoder_input_sequences = np.zeros_like(train_incorrect)\ntrain_decoder_input_sequences[:, 1:] = train_incorrect[:, :-1]\n\nval_decoder_input_sequences = np.zeros_like(val_incorrect)\nval_decoder_input_sequences[:, 1:] = val_incorrect[:, :-1]\n\n# Prepare target sequences for training (correct sequences)\ntrain_decoder_target_sequences = np.expand_dims(train_correct, -1)\nval_decoder_target_sequences = np.expand_dims(val_correct, -1)\n\n# Define model parameters\nembedding_dim = 64\nlatent_dim = 128\nnum_tokens = len(tokenizer.word_index) + 1  # +1 for padding token\n\n# Encoder\nencoder_inputs = Input(shape=(None,))\nencoder_embedding = Embedding(num_tokens, embedding_dim)(encoder_inputs)\nencoder_lstm = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\nencoder_states = [state_h, state_c]\n\n# Decoder\ndecoder_inputs = Input(shape=(None,))\ndecoder_embedding = Embedding(num_tokens, embedding_dim)(decoder_inputs)\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = Dense(num_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Seq2Seq Model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(\n    [train_incorrect, train_decoder_input_sequences],\n    train_decoder_target_sequences,\n    epochs=50,\n    batch_size=64,\n    validation_data=([val_incorrect, val_decoder_input_sequences], val_decoder_target_sequences)\n)\n\n# Evaluate the model on the validation data\nval_loss, val_accuracy = model.evaluate([val_incorrect, val_decoder_input_sequences], val_decoder_target_sequences)\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T13:08:26.103255Z","iopub.execute_input":"2024-07-17T13:08:26.104006Z","iopub.status.idle":"2024-07-17T13:34:20.239175Z","shell.execute_reply.started":"2024-07-17T13:08:26.103977Z","shell.execute_reply":"2024-07-17T13:34:20.238353Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 9ms/step - accuracy: 0.7655 - loss: 0.8888 - val_accuracy: 0.8471 - val_loss: 0.5193\nEpoch 2/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.8603 - loss: 0.4693 - val_accuracy: 0.8897 - val_loss: 0.3611\nEpoch 3/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.8965 - loss: 0.3375 - val_accuracy: 0.9076 - val_loss: 0.3009\nEpoch 4/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9122 - loss: 0.2833 - val_accuracy: 0.9181 - val_loss: 0.2639\nEpoch 5/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9205 - loss: 0.2548 - val_accuracy: 0.9229 - val_loss: 0.2463\nEpoch 6/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9263 - loss: 0.2353 - val_accuracy: 0.9267 - val_loss: 0.2335\nEpoch 7/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9311 - loss: 0.2196 - val_accuracy: 0.9303 - val_loss: 0.2226\nEpoch 8/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9350 - loss: 0.2072 - val_accuracy: 0.9315 - val_loss: 0.2184\nEpoch 9/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9377 - loss: 0.1971 - val_accuracy: 0.9358 - val_loss: 0.2049\nEpoch 10/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9404 - loss: 0.1886 - val_accuracy: 0.9375 - val_loss: 0.2003\nEpoch 11/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9429 - loss: 0.1808 - val_accuracy: 0.9384 - val_loss: 0.1949\nEpoch 12/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 9ms/step - accuracy: 0.9445 - loss: 0.1756 - val_accuracy: 0.9395 - val_loss: 0.1914\nEpoch 13/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9462 - loss: 0.1705 - val_accuracy: 0.9400 - val_loss: 0.1909\nEpoch 14/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9477 - loss: 0.1653 - val_accuracy: 0.9423 - val_loss: 0.1839\nEpoch 15/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9491 - loss: 0.1616 - val_accuracy: 0.9419 - val_loss: 0.1845\nEpoch 16/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9502 - loss: 0.1577 - val_accuracy: 0.9441 - val_loss: 0.1803\nEpoch 17/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9512 - loss: 0.1546 - val_accuracy: 0.9437 - val_loss: 0.1801\nEpoch 18/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9520 - loss: 0.1519 - val_accuracy: 0.9438 - val_loss: 0.1809\nEpoch 19/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9532 - loss: 0.1483 - val_accuracy: 0.9441 - val_loss: 0.1789\nEpoch 20/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9540 - loss: 0.1465 - val_accuracy: 0.9456 - val_loss: 0.1768\nEpoch 21/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9544 - loss: 0.1451 - val_accuracy: 0.9454 - val_loss: 0.1791\nEpoch 22/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9555 - loss: 0.1417 - val_accuracy: 0.9454 - val_loss: 0.1778\nEpoch 23/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9560 - loss: 0.1399 - val_accuracy: 0.9461 - val_loss: 0.1765\nEpoch 24/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9568 - loss: 0.1377 - val_accuracy: 0.9451 - val_loss: 0.1785\nEpoch 25/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9574 - loss: 0.1362 - val_accuracy: 0.9461 - val_loss: 0.1763\nEpoch 26/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9580 - loss: 0.1340 - val_accuracy: 0.9466 - val_loss: 0.1776\nEpoch 27/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9586 - loss: 0.1324 - val_accuracy: 0.9459 - val_loss: 0.1762\nEpoch 28/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9587 - loss: 0.1319 - val_accuracy: 0.9465 - val_loss: 0.1767\nEpoch 29/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9589 - loss: 0.1314 - val_accuracy: 0.9468 - val_loss: 0.1766\nEpoch 30/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9597 - loss: 0.1292 - val_accuracy: 0.9474 - val_loss: 0.1767\nEpoch 31/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9604 - loss: 0.1271 - val_accuracy: 0.9469 - val_loss: 0.1816\nEpoch 32/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9607 - loss: 0.1260 - val_accuracy: 0.9466 - val_loss: 0.1789\nEpoch 33/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step - accuracy: 0.9609 - loss: 0.1254 - val_accuracy: 0.9482 - val_loss: 0.1743\nEpoch 34/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9614 - loss: 0.1236 - val_accuracy: 0.9469 - val_loss: 0.1788\nEpoch 35/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9617 - loss: 0.1230 - val_accuracy: 0.9473 - val_loss: 0.1782\nEpoch 36/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9617 - loss: 0.1228 - val_accuracy: 0.9474 - val_loss: 0.1791\nEpoch 37/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9621 - loss: 0.1218 - val_accuracy: 0.9463 - val_loss: 0.1790\nEpoch 38/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9628 - loss: 0.1194 - val_accuracy: 0.9476 - val_loss: 0.1802\nEpoch 39/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9632 - loss: 0.1186 - val_accuracy: 0.9480 - val_loss: 0.1829\nEpoch 40/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9632 - loss: 0.1187 - val_accuracy: 0.9475 - val_loss: 0.1820\nEpoch 41/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9638 - loss: 0.1167 - val_accuracy: 0.9463 - val_loss: 0.1846\nEpoch 42/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9636 - loss: 0.1172 - val_accuracy: 0.9482 - val_loss: 0.1788\nEpoch 43/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9641 - loss: 0.1159 - val_accuracy: 0.9477 - val_loss: 0.1829\nEpoch 44/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9644 - loss: 0.1152 - val_accuracy: 0.9475 - val_loss: 0.1850\nEpoch 45/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - accuracy: 0.9647 - loss: 0.1141 - val_accuracy: 0.9476 - val_loss: 0.1845\nEpoch 46/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9648 - loss: 0.1134 - val_accuracy: 0.9480 - val_loss: 0.1812\nEpoch 47/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9651 - loss: 0.1126 - val_accuracy: 0.9475 - val_loss: 0.1842\nEpoch 48/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9653 - loss: 0.1125 - val_accuracy: 0.9473 - val_loss: 0.1862\nEpoch 49/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9657 - loss: 0.1110 - val_accuracy: 0.9478 - val_loss: 0.1857\nEpoch 50/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9658 - loss: 0.1111 - val_accuracy: 0.9481 - val_loss: 0.1854\n\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9471 - loss: 0.1880\nValidation Accuracy: 0.9481\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('phoneme_correction_model_editDistance_accuracy94.h5')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T05:27:45.739692Z","iopub.execute_input":"2024-09-13T05:27:45.740093Z","iopub.status.idle":"2024-09-13T05:27:45.762691Z","shell.execute_reply.started":"2024-09-13T05:27:45.740065Z","shell.execute_reply":"2024-09-13T05:27:45.761959Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the model\nloaded_model = load_model('/kaggle/input/accuracy94/phoneme_correction_model_editDistance_accuracy94.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T04:43:02.837561Z","iopub.execute_input":"2024-09-17T04:43:02.838413Z","iopub.status.idle":"2024-09-17T04:43:02.942213Z","shell.execute_reply.started":"2024-09-17T04:43:02.838379Z","shell.execute_reply":"2024-09-17T04:43:02.941458Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"new_mispronounced_sequence = \"\"  # Replace with actual sequence\nnew_mispronounced_sequence_tokenized = tokenizer.texts_to_sequences([new_mispronounced_sequence])\nnew_mispronounced_sequence_padded = pad_sequences(new_mispronounced_sequence_tokenized, maxlen=max_sequence_length, padding='post')\n\n# Prepare the decoder input for inference\ndecoder_input = np.zeros_like(new_mispronounced_sequence_padded)\ndecoder_input[:, 1:] = new_mispronounced_sequence_padded[:, :-1]\n\n# Predict\npredicted_phonemes = loaded_model.predict([new_mispronounced_sequence_padded, decoder_input])","metadata":{"execution":{"iopub.status.busy":"2024-08-25T10:09:48.225691Z","iopub.execute_input":"2024-08-25T10:09:48.226485Z","iopub.status.idle":"2024-08-25T10:09:49.785601Z","shell.execute_reply.started":"2024-08-25T10:09:48.226451Z","shell.execute_reply":"2024-08-25T10:09:49.784830Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the token with the highest probability for each timestep\npredicted_token_indices = np.argmax(predicted_phonemes, axis=-1)\n\n# Create a reverse mapping from index to phoneme\nindex_to_phoneme = {index: phoneme for phoneme, index in tokenizer.word_index.items()}\n\n# Add the padding token to the reverse mapping\nindex_to_phoneme[0] = ''  # Assuming 0 is the padding token\n\n# Decode the predicted token indices to phoneme sequences\npredicted_phoneme_sequences = []\nfor token_sequence in predicted_token_indices:\n    predicted_phoneme_sequence = ''.join([index_to_phoneme[token] for token in token_sequence])\n    predicted_phoneme_sequences.append(predicted_phoneme_sequence)\n\n# Print the decoded phoneme sequence\nfor seq in predicted_phoneme_sequences:\n    print(seq)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T10:09:52.583588Z","iopub.execute_input":"2024-08-25T10:09:52.583942Z","iopub.status.idle":"2024-08-25T10:09:52.591582Z","shell.execute_reply.started":"2024-08-25T10:09:52.583915Z","shell.execute_reply":"2024-08-25T10:09:52.590675Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"plˈeɪɡ\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the model\nloaded_model = load_model('/kaggle/input/accuracy94/phoneme_correction_model_editDistance_accuracy94.h5')\n\n# List of new mispronounced sequences\nnew_mispronounced_sequences = [\n  \"kˈæməflˌɑːʒɛ\"\n\n]\n\n# Function to predict and decode the sequence\ndef predict_and_decode(sequence):\n    # Tokenize and pad the sequence\n    tokenized_sequence = tokenizer.texts_to_sequences([sequence])\n    padded_sequence = pad_sequences(tokenized_sequence, maxlen=max_sequence_length, padding='post')\n\n    # Prepare the decoder input for inference\n    decoder_input = np.zeros_like(padded_sequence)\n    decoder_input[:, 1:] = padded_sequence[:, :-1]\n\n    # Predict the phonemes\n    predicted_phonemes = loaded_model.predict([padded_sequence, decoder_input])\n    \n    # Get the token with the highest probability for each timestep\n    predicted_token_indices = np.argmax(predicted_phonemes, axis=-1)\n\n    # Create a reverse mapping from index to phoneme\n    index_to_phoneme = {index: phoneme for phoneme, index in tokenizer.word_index.items()}\n    index_to_phoneme[0] = ''  # Assuming 0 is the padding token\n\n    # Decode the predicted token indices to phoneme sequence\n    predicted_phoneme_sequence = ''.join([index_to_phoneme[token] for token in predicted_token_indices[0]])\n\n    return predicted_phoneme_sequence\n\n# Iterate over the list of mispronounced sequences and get the corrected output for each\ncorrected_phoneme_sequences = [predict_and_decode(seq) for seq in new_mispronounced_sequences]\n\n# Print the decoded phoneme sequences\nfor original, corrected in zip(new_mispronounced_sequences, corrected_phoneme_sequences):\n    print(f\"Original: {original} -> Corrected: {corrected}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T15:26:50.097240Z","iopub.execute_input":"2024-09-25T15:26:50.098090Z","iopub.status.idle":"2024-09-25T15:26:50.459234Z","shell.execute_reply.started":"2024-09-25T15:26:50.098056Z","shell.execute_reply":"2024-09-25T15:26:50.458266Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\nOriginal: kˈæməflˌɑːʒɛ -> Corrected: kˈæməllˌɑːd\n","output_type":"stream"}]},{"cell_type":"code","source":"# phoneme correction with BILSTM - accuracy 95%","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, TimeDistributed\nimport numpy as np\n\n# Load the CSV file\nfile_path = '/kaggle/input/correct/capstone_dataset_hugging_face.csv'  # Replace with the path to your CSV file\ndf = pd.read_csv(file_path)\n\n# Combine all phoneme sequences to fit the tokenizer\nall_phonemes = list(df['ipa_phoneme']) + list(df['incorrect_ipa_phoneme'])\n\ntokenizer = Tokenizer(char_level=True)  # Use character-level tokenization\ntokenizer.fit_on_texts(all_phonemes)\n\n# Create tokenized sequences\ncorrect_sequences = tokenizer.texts_to_sequences(df['ipa_phoneme'])\nincorrect_sequences = tokenizer.texts_to_sequences(df['incorrect_ipa_phoneme'])\n\n# Pad sequences to the same length\nmax_sequence_length = max(max(len(seq) for seq in correct_sequences), max(len(seq) for seq in incorrect_sequences))\n\ncorrect_sequences = pad_sequences(correct_sequences, maxlen=max_sequence_length, padding='post')\nincorrect_sequences = pad_sequences(incorrect_sequences, maxlen=max_sequence_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:03:48.750500Z","iopub.execute_input":"2024-09-26T03:03:48.750933Z","iopub.status.idle":"2024-09-26T03:04:01.178046Z","shell.execute_reply.started":"2024-09-26T03:03:48.750893Z","shell.execute_reply":"2024-09-26T03:04:01.176846Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n\n# Split the data into training and validation sets\ntrain_incorrect, val_incorrect, train_correct, val_correct = train_test_split(\n    incorrect_sequences, correct_sequences, test_size=0.2, random_state=42\n)\n\n# Define model parameters\nembedding_dim = 64\nlatent_dim = 128\nnum_tokens = len(tokenizer.word_index) + 1  # +1 for padding token\n\n# Input layer for incorrect phoneme sequences\ninputs = Input(shape=(max_sequence_length,))\n\n# Embedding layer\nembedding_layer = Embedding(num_tokens, embedding_dim,\ninput_length=max_sequence_length)(inputs)\n\n# BiLSTM layer\nbilstm_layer = Bidirectional(LSTM(latent_dim, return_sequences=True))\n(embedding_layer)\n\n# TimeDistributed Dense layer to output a corrected phoneme at each time step\noutput_layer = TimeDistributed(Dense(num_tokens, activation='softmax'))\n(bilstm_layer)\n\n# Define the model\nmodel = Model(inputs, output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Expand the correct sequences to match the expected output shape\ntrain_correct_expanded = np.expand_dims(train_correct, -1)\nval_correct_expanded = np.expand_dims(val_correct, -1)\n\n# Train the model\nhistory = model.fit(\n    train_incorrect,\n    train_correct_expanded,\n    epochs=50,\n    batch_size=64,\n    validation_data=(val_incorrect, val_correct_expanded)\n)\n\n# Evaluate the model on the validation data\nval_loss, val_accuracy = model.evaluate(val_incorrect, val_correct_expanded)\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-13T04:27:28.364291Z","iopub.execute_input":"2024-09-13T04:27:28.364648Z","iopub.status.idle":"2024-09-13T05:03:13.012215Z","shell.execute_reply.started":"2024-09-13T04:27:28.364621Z","shell.execute_reply":"2024-09-13T05:03:13.011167Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-09-13 04:27:30.994905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-13 04:27:30.995014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-13 04:27:31.118819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 12ms/step - accuracy: 0.8585 - loss: 0.5285 - val_accuracy: 0.9284 - val_loss: 0.2134\nEpoch 2/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9332 - loss: 0.2002 - val_accuracy: 0.9402 - val_loss: 0.1771\nEpoch 3/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9447 - loss: 0.1672 - val_accuracy: 0.9461 - val_loss: 0.1621\nEpoch 4/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9490 - loss: 0.1533 - val_accuracy: 0.9486 - val_loss: 0.1548\nEpoch 5/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 12ms/step - accuracy: 0.9519 - loss: 0.1445 - val_accuracy: 0.9495 - val_loss: 0.1516\nEpoch 6/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9540 - loss: 0.1381 - val_accuracy: 0.9510 - val_loss: 0.1476\nEpoch 7/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9555 - loss: 0.1334 - val_accuracy: 0.9520 - val_loss: 0.1453\nEpoch 8/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9568 - loss: 0.1295 - val_accuracy: 0.9511 - val_loss: 0.1460\nEpoch 9/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9580 - loss: 0.1265 - val_accuracy: 0.9521 - val_loss: 0.1451\nEpoch 10/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9589 - loss: 0.1236 - val_accuracy: 0.9528 - val_loss: 0.1452\nEpoch 11/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9597 - loss: 0.1210 - val_accuracy: 0.9529 - val_loss: 0.1434\nEpoch 12/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9605 - loss: 0.1184 - val_accuracy: 0.9536 - val_loss: 0.1412\nEpoch 13/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9611 - loss: 0.1172 - val_accuracy: 0.9528 - val_loss: 0.1425\nEpoch 14/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9618 - loss: 0.1150 - val_accuracy: 0.9533 - val_loss: 0.1420\nEpoch 15/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9623 - loss: 0.1136 - val_accuracy: 0.9534 - val_loss: 0.1449\nEpoch 16/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9628 - loss: 0.1123 - val_accuracy: 0.9533 - val_loss: 0.1437\nEpoch 17/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9634 - loss: 0.1109 - val_accuracy: 0.9538 - val_loss: 0.1425\nEpoch 18/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9637 - loss: 0.1098 - val_accuracy: 0.9534 - val_loss: 0.1448\nEpoch 19/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9641 - loss: 0.1086 - val_accuracy: 0.9536 - val_loss: 0.1438\nEpoch 20/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9646 - loss: 0.1073 - val_accuracy: 0.9538 - val_loss: 0.1448\nEpoch 21/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9649 - loss: 0.1066 - val_accuracy: 0.9533 - val_loss: 0.1459\nEpoch 22/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9653 - loss: 0.1051 - val_accuracy: 0.9537 - val_loss: 0.1443\nEpoch 23/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9659 - loss: 0.1037 - val_accuracy: 0.9536 - val_loss: 0.1458\nEpoch 24/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9657 - loss: 0.1042 - val_accuracy: 0.9536 - val_loss: 0.1468\nEpoch 25/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9659 - loss: 0.1036 - val_accuracy: 0.9538 - val_loss: 0.1466\nEpoch 26/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9664 - loss: 0.1023 - val_accuracy: 0.9537 - val_loss: 0.1477\nEpoch 27/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9668 - loss: 0.1014 - val_accuracy: 0.9541 - val_loss: 0.1474\nEpoch 28/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9669 - loss: 0.1011 - val_accuracy: 0.9530 - val_loss: 0.1495\nEpoch 29/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9673 - loss: 0.0999 - val_accuracy: 0.9537 - val_loss: 0.1501\nEpoch 30/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9674 - loss: 0.0995 - val_accuracy: 0.9537 - val_loss: 0.1491\nEpoch 31/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9676 - loss: 0.0988 - val_accuracy: 0.9536 - val_loss: 0.1503\nEpoch 32/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9677 - loss: 0.0987 - val_accuracy: 0.9536 - val_loss: 0.1506\nEpoch 33/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9682 - loss: 0.0972 - val_accuracy: 0.9530 - val_loss: 0.1518\nEpoch 34/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9681 - loss: 0.0979 - val_accuracy: 0.9536 - val_loss: 0.1508\nEpoch 35/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9684 - loss: 0.0965 - val_accuracy: 0.9537 - val_loss: 0.1519\nEpoch 36/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9686 - loss: 0.0961 - val_accuracy: 0.9535 - val_loss: 0.1524\nEpoch 37/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0958 - val_accuracy: 0.9537 - val_loss: 0.1534\nEpoch 38/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9689 - loss: 0.0954 - val_accuracy: 0.9533 - val_loss: 0.1549\nEpoch 39/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9687 - loss: 0.0960 - val_accuracy: 0.9535 - val_loss: 0.1527\nEpoch 40/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9689 - loss: 0.0950 - val_accuracy: 0.9531 - val_loss: 0.1556\nEpoch 41/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9692 - loss: 0.0949 - val_accuracy: 0.9529 - val_loss: 0.1558\nEpoch 42/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9696 - loss: 0.0935 - val_accuracy: 0.9534 - val_loss: 0.1541\nEpoch 43/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9695 - loss: 0.0938 - val_accuracy: 0.9530 - val_loss: 0.1574\nEpoch 44/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9695 - loss: 0.0935 - val_accuracy: 0.9533 - val_loss: 0.1550\nEpoch 45/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9697 - loss: 0.0935 - val_accuracy: 0.9531 - val_loss: 0.1565\nEpoch 46/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9701 - loss: 0.0922 - val_accuracy: 0.9532 - val_loss: 0.1571\nEpoch 47/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9700 - loss: 0.0921 - val_accuracy: 0.9535 - val_loss: 0.1568\nEpoch 48/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9700 - loss: 0.0924 - val_accuracy: 0.9534 - val_loss: 0.1583\nEpoch 49/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9701 - loss: 0.0921 - val_accuracy: 0.9533 - val_loss: 0.1585\nEpoch 50/50\n\u001b[1m3437/3437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.9706 - loss: 0.0909 - val_accuracy: 0.9533 - val_loss: 0.1568\n\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9525 - loss: 0.1596\nValidation Accuracy: 0.9533\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('phoneme_correction_bilstm_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T05:05:26.340068Z","iopub.execute_input":"2024-09-13T05:05:26.340425Z","iopub.status.idle":"2024-09-13T05:05:26.375785Z","shell.execute_reply.started":"2024-09-13T05:05:26.340398Z","shell.execute_reply":"2024-09-13T05:05:26.374967Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the model from the saved .h5 file\nmodel = load_model('/kaggle/input/bilstm-correction-model/phoneme_correction_bilstm_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:02:25.674959Z","iopub.execute_input":"2024-09-26T03:02:25.675436Z","iopub.status.idle":"2024-09-26T03:02:41.467545Z","shell.execute_reply.started":"2024-09-26T03:02:25.675401Z","shell.execute_reply":"2024-09-26T03:02:41.466190Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-09-26 03:02:27.713816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-26 03:02:27.713975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-26 03:02:27.872486: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\n\n\n# Example phoneme input (replace this with the phoneme sequence you want to test)\ninput_phoneme_sequence = 'nŋˈɑːvəl'\n\n# Tokenize the phoneme sequence\ninput_sequence = tokenizer.texts_to_sequences([input_phoneme_sequence])\n\n# Pad the sequence to the maximum length used during training\ninput_sequence_padded = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n\n# Make a prediction\npredicted_sequence = model.predict(input_sequence_padded)\n\n# Get the predicted token for each time step (output is probabilities, so we take argmax)\npredicted_token_ids = np.argmax(predicted_sequence, axis=-1)\n\n# Convert token IDs back to phonemes\npredicted_phonemes = [tokenizer.index_word[token] for token in predicted_token_ids[0] if token > 0]\n\n# Join the predicted phonemes into a readable string\npredicted_phoneme_sequence = ' '.join(predicted_phonemes)\n\n# Output the predicted phoneme sequence\nprint(\"Predicted corrected phoneme sequence:\", predicted_phoneme_sequence)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:14:25.071652Z","iopub.execute_input":"2024-09-26T03:14:25.072640Z","iopub.status.idle":"2024-09-26T03:14:25.150697Z","shell.execute_reply.started":"2024-09-26T03:14:25.072605Z","shell.execute_reply":"2024-09-26T03:14:25.149693Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nPredicted corrected phoneme sequence: n ˈ ɑ ː v ə l\n","output_type":"stream"}]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition\n",
        "\n",
        "# Install eSpeak\n",
        "!sudo apt-get install espeak\n",
        "\n",
        "# Example of using eSpeak for phoneme conversion\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FUFz6uwQnnj",
        "outputId": "c5ca53f4-7e3c-4237-da37-b5d0a151feb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
            "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,382 kB of archives.\n",
            "After this operation, 3,178 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak-data amd64 1.48.15+dfsg-3 [1,085 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libespeak1 amd64 1.48.15+dfsg-3 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak amd64 1.48.15+dfsg-3 [64.2 kB]\n",
            "Fetched 1,382 kB in 1s (1,580 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak (1.48.15+dfsg-3) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up espeak (1.48.15+dfsg-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "google api  + espeak"
      ],
      "metadata": {
        "id": "LcQIaR9mfHSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B79Vt7jOQioF",
        "outputId": "73060226-fe76-4efa-b343-f459be185d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed Text:  Tatum\n",
            "IPA Transcription:  tˈɑːtəm\n"
          ]
        }
      ],
      "source": [
        "import speech_recognition as sr\n",
        "import subprocess\n",
        "\n",
        "# Initialize recognizer\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Load audio file\n",
        "audio_file = \"/content/record_out (3).wav\"\n",
        "\n",
        "with sr.AudioFile(audio_file) as source:\n",
        "    audio_data = recognizer.record(source)\n",
        "    # Recognize speech using Google Web Speech API\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio_data)\n",
        "        print(\"Transcribed Text: \", text)\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Web Speech API could not understand audio\")\n",
        "        text = \"\"\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google Web Speech API; {e}\")\n",
        "        text = \"\"\n",
        "\n",
        "# Use espeak to convert text to IPA\n",
        "if text:\n",
        "    espeak_command = f'espeak --ipa \"{text}\"'\n",
        "    process = subprocess.Popen(espeak_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    ipa_transcription = stdout.decode('utf-8').strip()\n",
        "    print(\"IPA Transcription: \", ipa_transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wav2vec model + espeak"
      ],
      "metadata": {
        "id": "TIN9gVcvfJ9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import subprocess\n",
        "\n",
        "# Load the audio file\n",
        "audio_file = \"/content/p_28781562_829.wav\"\n",
        "\n",
        "# Load Wav2Vec2 processor and model\n",
        "model_name = \"facebook/wav2vec2-large-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "\n",
        "# Load the audio file and resample to 16kHz\n",
        "waveform, sample_rate = torchaudio.load(audio_file)\n",
        "waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "\n",
        "# Process the audio input\n",
        "input_values = processor(waveform.squeeze(), return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "# Get the predicted IDs and decode them to text\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcribed_text = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "print(\"Transcribed Text: \", transcribed_text)\n",
        "\n",
        "# Use espeak to convert text to IPA\n",
        "if transcribed_text:\n",
        "    espeak_command = f'espeak --ipa \"{transcribed_text}\"'\n",
        "    process = subprocess.Popen(espeak_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    ipa_transcription = stdout.decode('utf-8').strip()\n",
        "    print(\"IPA Transcription: \", ipa_transcription)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ3-KR2kgzyf",
        "outputId": "dc773e73-9419-4859-fb55-b97d80783a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed Text:  EXPRESSO\n",
            "IPA Transcription:  ɛkspɹˈɛsəʊ\n"
          ]
        }
      ]
    }
  ]
}